<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>Testing</title>
<link rel="stylesheet" href="../../boostbook.css" type="text/css">
<meta name="generator" content="DocBook XSL Stylesheets V1.69.1">
<link rel="start" href="../../index.html" title="Boost.Build V2 User Manual">
<link rel="up" href="../tasks.html" title="Chapter 5. Common tasks">
<link rel="prev" href="../tasks/installing.html" title="Installing">
<link rel="next" href="raw.html" title="Custom commands">
</head>
<body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF">
<table cellpadding="2" width="100%"><tr><td valign="top"><img alt="Boost C++ Libraries" width="277" height="86" src="../../../../boost.png"></td></tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../tasks/installing.html"><img src="../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../tasks.html"><img src="../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="raw.html"><img src="../../../../doc/html/images/next.png" alt="Next"></a>
</div>
<div class="section" lang="en">
<div class="titlepage"><div><div><h2 class="title" style="clear: both">
<a name="bbv2.builtins.testing"></a>Testing</h2></div></div></div>
<p>Boost.Build has convenient support for running unit tests. The
        simplest way is the <code class="computeroutput">unit-test</code> rule, which follows the
        <a href="../advanced/targets.html#bbv2.main-target-rule-syntax">common syntax</a>. For
        example:
</p>
<pre class="programlisting">
unit-test helpers_test : helpers_test.cpp helpers ;
</pre>
<p>
      </p>
<p>The <code class="computeroutput">unit-test</code> rule behaves like the
        <code class="computeroutput">exe</code> rule, but after the executable is created it is
        run. If the executable returns an error code, the build system will also
        return an error and will try running the executable on the next
        invocation until it runs successfully. This behaviour ensures that you
        can't miss a unit test failure.
      </p>
<p>By default, the executable is run directly. Sometimes, it's 
      desirable to run the executable using some helper command. You should use the
      <code class="literal">testing.launcher</code> property to specify the name of the
      helper command. For example, if you write:
      </p>
<pre class="programlisting">
unit-test helpers_test 
   : helpers_test.cpp helpers 
   : <span class="bold"><strong>&lt;testing.launcher&gt;valgrind</strong></span>
   ;  
</pre>
<p>The command used to run the executable will be:</p>
<pre class="screen">
<span class="bold"><strong>valgrind</strong></span> bin/$toolset/debug/helpers_test 
</pre>
<p>There are few specialized testing rules, listed below:
      </p>
<pre class="programlisting">
rule compile ( sources : requirements * : target-name ? )
rule compile-fail ( sources : requirements * : target-name ? )
rule link ( sources + : requirements * : target-name ? )
rule link-fail ( sources + : requirements * : target-name ? )
      </pre>
<p>
      They are are given a list of sources and requirements. 
      If the target name is not provided, the name of the first
      source file is used instead. The <code class="literal">compile*</code>
      tests try to compile the passed source. The <code class="literal">link*</code>
      rules try to compile and link an application from all the passed sources.
      The <code class="literal">compile</code> and <code class="literal">link</code> rules expect
      that compilation/linking succeeds. The <code class="literal">compile-fail</code>
      and <code class="literal">link-fail</code> rules, on the opposite, expect that
      the compilation/linking fails.
      </p>
<p>There are two specialized rules for running applications, which
      are more powerful than the <code class="computeroutput">unit-test</code> rule. The 
      <code class="computeroutput">run</code> rule has the following signature:
      </p>
<pre class="programlisting">
rule run ( sources + : args * : input-files * : requirements * : target-name ? 
    : default-build * )
      </pre>
<p>
      The rule builds application from the provided sources and runs it,
      passing <code class="varname">args</code> and <code class="varname">input-files</code>
      as command-line arguments. The <code class="varname">args</code> parameter
      is passed verbatim and the values of the <code class="varname">input-files</code>
      parameter are treated as paths relative to containing Jamfile, and are
      adjusted if <span><strong class="command">bjam</strong></span> is invoked from a different
      directory. The <code class="computeroutput">run-fail</code> rule is identical to the
      <code class="computeroutput">run</code> rule, except that it expects that the run fails.
      </p>
<p>All rules described in this section, if executed successfully,
      create a special manifest file to indicate that the test passed.
      For the <code class="computeroutput">unit-test</code> rule the files is named
      <code class="filename"><em class="replaceable"><code>target-name</code></em>.passed</code> and
      for the other rules it is called 
      <code class="filename"><em class="replaceable"><code>target-name</code></em>.test</code>.
      The <code class="computeroutput">run*</code> rules also capture all output from the program,
      and store it in a file named 
      <code class="filename"><em class="replaceable"><code>target-name</code></em>.output</code>.</p>
<p>The <code class="computeroutput">run</code> and the <code class="computeroutput">run-fail</code> rules, if
      the test passes, automatically delete the linked executable, to
      save space. This behaviour can be suppressed by passing the
      <code class="literal">--preserve-test-targets</code> command line option.</p>
<p>It is possible to print the list of all test targets (except for
      <code class="computeroutput">unit-test</code>) declared in your project, by passing
      the <code class="literal">--dump-tests</code> command-line option. The output
      will consist of lines of the form:
      </p>
<pre class="screen">
boost-test(<em class="replaceable"><code>test-type</code></em>) <em class="replaceable"><code>path</code></em> : <em class="replaceable"><code>sources</code></em> 
      </pre>
<p>      
      </p>
<p>It is possible to process the list of tests, the output of
      bjam during command run, and the presense/absense of the 
      <code class="filename">*.test</code> files created when test passes into
      human-readable status table of tests. Such processing utilities
      are not included in Boost.Build.</p>
</div>
<table xmlns:rev="http://www.cs.rpi.edu/~gregod/boost/tools/doc/revision" width="100%"><tr>
<td align="left"></td>
<td align="right"><small></small></td>
</tr></table>
<hr>
<div class="spirit-nav">
<a accesskey="p" href="../tasks/installing.html"><img src="../../../../doc/html/images/prev.png" alt="Prev"></a><a accesskey="u" href="../tasks.html"><img src="../../../../doc/html/images/up.png" alt="Up"></a><a accesskey="h" href="../../index.html"><img src="../../../../doc/html/images/home.png" alt="Home"></a><a accesskey="n" href="raw.html"><img src="../../../../doc/html/images/next.png" alt="Next"></a>
</div>
</body>
</html>
